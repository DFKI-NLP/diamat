[PREPROCESSING]
source = ./data/input/source.txt
word_vectors_source = ./data/input/word_vectors/wiki.de.vec
vocab_size_source = 50000
embedding_dim_source = 300
token2idx_source = ./data/output/token2idx_source.p
idx2vec_source = ./data/output/idx2vec_source.p
human = ./data/input/human.txt
machine = ./data/input/machine.txt
word_vectors_target = ./data/input/word_vectors/wiki.en.vec
vocab_size_target = 50000
embedding_dim_target = 300
token2idx_target = ./data/output/token2idx_target.p
idx2vec_target = ./data/output/idx2vec_target.p
bundle = ./data/output/bundle.jsonl
prefix = -1
sequence_length = 150

[TRAINING]
embedding_dim_source = 300
idx2vec_source = ./data/output/idx2vec_source.p
embedding_dim_target = 300
idx2vec_target = ./data/output/idx2vec_target.p
sequence_length = 150
training_doc = ./data/output/train.jsonl
val_doc = ./data/output/val.jsonl
num_filters = 150
filter_sizes = 2,3,4
drop = .5
batch_size = 64
epochs = 500
model_params = ./data/output/model_params.hdf5
hyper_params = ./data/output/model_hyper_params.p
max_queue_size = 80 
use_multiprocessing = True
workers = 16

[EXPLANATION]
idx2vec_source = ./data/output/idx2vec_source.p
embedding_dim_source = 300
idx2vec_target = ./data/output/idx2vec_target.p
embedding_dim_target = 300
sequence_length = 150
num_filters = 150
drop = .5
have_activation = False
model_params = ./data/output/model_params.hdf5
filter_sizes = 2,3,4
analyser_name = pattern.attribution
explain_doc = ./data/output/explain.jsonl
batch_size = 64
train_doc = ./data/output/xtrain.jsonl
out_file = ./data/output/explain.jsonl
max_abs_contribution = 1.4913137
